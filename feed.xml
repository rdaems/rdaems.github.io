<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://rdaems.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rdaems.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-31T15:18:13+00:00</updated><id>https://rdaems.github.io/feed.xml</id><title type="html">blank</title><subtitle>Welcome to my personal website! </subtitle><entry><title type="html">Variational Inference for SDEs Driven by Fractional Noise</title><link href="https://rdaems.github.io/blog/2024/ma-fbm/" rel="alternate" type="text/html" title="Variational Inference for SDEs Driven by Fractional Noise"/><published>2024-04-22T12:00:00+00:00</published><updated>2024-04-22T12:00:00+00:00</updated><id>https://rdaems.github.io/blog/2024/ma-fbm</id><content type="html" xml:base="https://rdaems.github.io/blog/2024/ma-fbm/"><![CDATA[<p><br/> <a href="https://openreview.net/forum?id=rtx8B94JMS" class="btn btn-primary" style="color: white" role="button">paper</a> <a href="https://videoneuralsde.github.io/" class="btn btn-primary" style="color: white" role="button">project page</a></p> <p>We present the first variational inference framework for non-Markovian neural stochastic differential equations (SDEs) driven by fractional Brownian Motion (fBM). Our method builds upon the idea of approximating the fBM by a linear combination of Markov processes, driven by the same, Brownian motion. We then provide the variational prior and posterior, as well as the ELBO.</p>]]></content><author><name>Rembert Daems</name></author><category term="research"/><summary type="html"><![CDATA[paper project page]]></summary></entry><entry><title type="html">KeyCLD</title><link href="https://rdaems.github.io/keycld/" rel="alternate" type="text/html" title="KeyCLD"/><published>2022-06-10T09:55:00+00:00</published><updated>2022-06-10T09:55:00+00:00</updated><id>https://rdaems.github.io/keycld</id><content type="html" xml:base="https://rdaems.github.io/keycld/"><![CDATA[<p><em>Rembert Daems, Jeroen Taets, Francis wyffels, Guillaume Crevecoeur</em></p> <p>This paper was published in <a href="https://www.sciencedirect.com/science/article/pii/S0925231223012985">Neurocomputing 573 (2024): 127175</a> and presented (oral, top 1.6%) at the Machine Learning and the Physical Sciences Workshop at NeurIPS 2023.</p> <p><a href="https://arxiv.org/abs/2206.11030" class="btn btn-primary" style="color: white" role="button">paper</a> <a href="https://github.com/rdaems/keycld" class="btn btn-primary" style="color: white" role="button">code</a> <a href="https://docs.google.com/presentation/d/1_y1RDOhyjaOTgPnXe5wxWfRcPq5K5Mhkp_GW-yM2NtA/edit?usp=sharing" class="btn btn-primary" style="color: white" role="button">slides</a></p> <h3 id="abstract">Abstract</h3> <p>We present KeyCLD, a framework to learn Lagrangian dynamics from images. Learned keypoints represent semantic landmarks in images and can directly represent state dynamics. Interpreting this state as Cartesian coordinates coupled with explicit holonomic constraints, allows expressing the dynamics with a constrained Lagrangian. KeyCLD is trained unsupervised end-to-end on sequences of images. Our method explicitly models the mass matrix, potential energy and the input matrix, thus allowing energy based control. We demonstrate learning of Lagrangian dynamics from images on the dm_control pendulum, cartpole and acrobot environments. We show that KeyCLD can be learned on these systems, wether they are unactuated, underactuated or fully actuated. Trained models are able to produce long-term video predictions, showing that the dynamics are accurately learned. We compare with Lag-VAE, Lag-caVAE and HGN, and ablations without constraints and without Lagrangian prior.</p> <figure> <video src="/assets/img/keycld/acrobot.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> <p>KeyCLD learns Lagrangian dynamics from images. <strong>(a)</strong> An observation of a dynamical system is processed by a keypoint estimator model. <strong>(b)</strong> The model represents the positions of the keypoints with a set of spatial probability heatmaps. <strong>(c)</strong> Cartesian coordinates are extracted using spatial softmax and used as state representations to learn Lagrangian dynamics. <strong>(d)</strong> The information in the keypoint coordinates bottleneck suffices for a learned renderer model to reconstruct the original observation, including background, reflections and shadows. The keypoint estimator model, Lagrangian dynamics models and renderer model are jointly learned unsupervised on sequences of images.</p> <h3 id="results">Results</h3> <p>KeyCLD predicts future frames for the pendulum, cartpole and acrobot environments. Every predicted sequence is based on the first three frames of the ground truth sequence (column 1) to estimate the velocities. KeyCLD (column 2) is capable of making accurate long-term predictions, including reflections and shadow. We compare these results with ablated models and related work in literature (other columns). See the <a href="https://arxiv.org/abs/2206.11030">paper</a> for more details.</p> <figure> <video src="/assets/img/keycld/pendulum-actuated_29.mp4" class="img-fluid z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> <figure> <video src="/assets/img/keycld/cartpole-actuated_4.mp4" class="img-fluid z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> <figure> <video src="/assets/img/keycld/acrobot-actuated_0.mp4" class="img-fluid z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> <p>Learning explicit energy models allow simple and robust energy shaping control. The videos below show we can reach a target state by leveraging the learned potential energy models, see the paper for more details.</p> <div style="display: flex; justify-content: space-between; gap: 10px;"> <div style="flex: 1; text-align: center;"> <figure> <video src="/assets/img/keycld/control/pendulum.mp4" class="img-fluid z-depth-1" width="80%" height="auto" autoplay="" controls=""/> </figure> </div> <div style="flex: 1; text-align: center;"> <figure> <video src="/assets/img/keycld/control/cartpole.mp4" class="img-fluid z-depth-1" width="80%" height="auto" autoplay="" controls=""/> </figure> </div> <div style="flex: 1; text-align: center;"> <figure> <video src="/assets/img/keycld/control/acrobot.mp4" class="img-fluid z-depth-1" width="80%" height="auto" autoplay="" controls=""/> </figure> </div> </div>]]></content><author><name>Rembert Daems</name></author><category term="research"/><summary type="html"><![CDATA[Rembert Daems, Jeroen Taets, Francis wyffels, Guillaume Crevecoeur]]></summary></entry><entry><title type="html">Sitting Posture Coach</title><link href="https://rdaems.github.io/blog/2021/posture-coach/" rel="alternate" type="text/html" title="Sitting Posture Coach"/><published>2021-10-18T12:00:00+00:00</published><updated>2021-10-18T12:00:00+00:00</updated><id>https://rdaems.github.io/blog/2021/posture-coach</id><content type="html" xml:base="https://rdaems.github.io/blog/2021/posture-coach/"><![CDATA[<p><a href="https://github.com/pderoovere/sitting-posture-coach" class="btn btn-primary" style="color: white" role="button">code</a> <a href="/sitting-posture-coach/" class="btn btn-primary" style="color: white" role="button">demo</a></p> <iframe src="https://player.vimeo.com/video/549610959?h=af4564a9ca" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen=""></iframe> <p>In the context of <a href="https://fullstackdeeplearning.com/">Full Stack Deep Learning</a> my friend <a href="https://twitter.com/peterderoovere">Peter</a> asked me to cooperate on his final project of the course. We came up with <strong>Sitting Posture Coach</strong> and were selected in the <a href="https://fullstackdeeplearning.com/spring2021/projects/">top 10 of 91 projects</a> by the course TAs.</p> <p>Maintaining a good sitting posture while working is extremely important. This is especially true when working from home, where ergonomic office equipment might not be available and there is less incentive to sit up straight. Bad sitting posture is a major cause of back pain, neck pain, headaches and even spinal disfunction.</p> <p>Sitting posture coach aims to provide a solution which is easy to set up and needs no additional infrastructure. A simple web page will provide feedback to help you attain a better sitting posture. It uses an AI system running locally in your browser (and thus preserving your privacy) that analyses live images from your webcam.</p> <p>A node.js app (running on Amazon LightSail) will serve web pages for inference and data collection. Collected data is stored in the cloud. Images are stored as objects (in an Amazon S3 bucket), the corresponding metadata is stored in a database (PostgreSQL). This data is used to train a classification model. The trained model is converted to run in the browser (TFJS) and made available by the node.js app.</p> <p>You can <a href="/sitting-posture-coach/">try out the app right now</a> or view the code and a more detailed technical explanation on <a href="https://github.com/pderoovere/sitting-posture-coach">github</a>!</p>]]></content><author><name>Rembert Daems</name></author><category term="side-project"/><summary type="html"><![CDATA[code demo]]></summary></entry><entry><title type="html">Unsupervised Orientation Learning Using Autoencoders</title><link href="https://rdaems.github.io/unsupervised-orientation/" rel="alternate" type="text/html" title="Unsupervised Orientation Learning Using Autoencoders"/><published>2020-12-01T12:00:00+00:00</published><updated>2020-12-01T12:00:00+00:00</updated><id>https://rdaems.github.io/unsupervised-orientation</id><content type="html" xml:base="https://rdaems.github.io/unsupervised-orientation/"><![CDATA[<p><a href="https://biblio.ugent.be/publication/8683423/file/8683425" class="btn btn-primary" style="color: white" role="button">paper</a> <a href="https://slideslive.com/38941768" class="btn btn-primary" style="color: white" role="button">slides</a></p> <p>I presented this work on the 2020 NeurIPS workshop on Differential Geometry meets Deep Learning.</p> <h3 id="abstract">Abstract</h3> <p>We present a method to learn the orientation of symmetric objects in real-world images in an unsupervised way. Our method explicitly maps in-plane relative rotations to the latent space of an autoencoder, by rotating both in the image domain and latent domain. This is achieved by adding a proposed crossing loss to a standard autoencoder training framework which enforces consistency between the image domain and latent domain rotations. This relative representation of rotation is made absolute, by using the symmetry of the observed object, resulting in an unsupervised method to learn the orientation. Furthermore, orientation is disentangled in latent space from other descriptive factors. We apply this method on two real-world datasets: aerial images of planes in the DOTA dataset and images of densely packed honeybees. We empirically show this method can learn orientation using no annotations with high accuracy compared to the same models trained with annotations.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/unsupervised-orientation/planes-480.webp 480w,/assets/img/unsupervised-orientation/planes-800.webp 800w,/assets/img/unsupervised-orientation/planes-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/unsupervised-orientation/planes.gif" class="img-fluid rounded z-depth-1" width="30%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure>]]></content><author><name>Rembert Daems</name></author><category term="research"/><summary type="html"><![CDATA[paper slides]]></summary></entry></feed>